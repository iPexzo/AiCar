import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import apiRouter from "./apis/routes/index";
import { body, validationResult } from "express-validator";
import OpenAI from "openai";
import { spawn } from "child_process";
import { promisify } from "util";
import { exec } from "child_process";
import net from "net";

// Load environment variables
dotenv.config();
console.log("Loaded OpenAI Key:", process.env.OPENAI_API_KEY?.slice(0, 12));

const app = express();
const PORT = process.env.PORT || 8001;
const HOST = process.env.HOST || "0.0.0.0";

// Enhanced CORS configuration for simulators
const corsOptions = {
  origin: [
    "http://localhost:3000",
    "http://localhost:8081",
    "http://localhost:8082",
    "http://localhost:19006",
    "exp://localhost:19000",
    "exp://192.168.8.149:19000",
    "http://10.0.2.2:8001",
    "http://10.0.2.2:3000",
    "http://10.0.2.2:8081",
    "http://10.0.2.2:8082",
    "http://192.168.8.149:8001",
    "http://192.168.8.149:3000",
    "http://192.168.8.149:8081",
    "http://192.168.8.149:8082",
  ],
  credentials: true,
  methods: ["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"],
  allowedHeaders: ["Content-Type", "Authorization", "X-Requested-With"],
};

// OpenAI setup
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Check if OpenAI API key is configured
if (!process.env.OPENAI_API_KEY) {
  console.warn("âš ï¸  WARNING: OPENAI_API_KEY is not configured!");
  console.warn("   The AI analysis will use a fallback template response.");
  console.warn(
    "   To enable real AI analysis, add your OpenAI API key to .env file"
  );
}

console.log("=== Car AI Backend Server Starting ===");
console.log("=== CORS enabled for all origins ===");

// Enable CORS with enhanced configuration for simulators
app.use(cors(corsOptions));

app.use(express.json({ limit: "10mb" }));
app.use(express.urlencoded({ extended: true }));

// Debug middleware to log all requests
app.use((req, res, next) => {
  console.log(`[DEBUG] ${req.method} ${req.originalUrl} - IP: ${req.ip}`);
  next();
});

// Health check endpoint
app.get("/health", (req, res) => {
  res.json({
    status: "OK",
    message: "Car AI Backend is running",
    timestamp: new Date().toISOString(),
  });
});

// Instead, use the consolidated router:
app.use("/api", apiRouter);

// Port checking and process management
async function checkAndKillPort(port: number): Promise<void> {
  return new Promise((resolve) => {
    console.log(`ğŸ” Checking if port ${port} is available...`);

    // Use kill-port package for more reliable port management
    const killPort = spawn("npx", ["kill-port", port.toString()], {
      shell: true,
      stdio: "pipe",
    });

    let output = "";
    let errorOutput = "";

    killPort.stdout.on("data", (data) => {
      output += data.toString();
    });

    killPort.stderr.on("data", (data) => {
      errorOutput += data.toString();
    });

    killPort.on("close", (code) => {
      if (code === 0) {
        console.log(`âœ… Port ${port} is now available`);
        // Wait a bit for the port to be fully released
        setTimeout(() => resolve(), 1000);
      } else {
        // Even if kill-port fails, we'll try to start the server anyway
        console.log(`âš ï¸  Port ${port} check completed (code: ${code})`);
        if (output) console.log(`Output: ${output.trim()}`);
        if (errorOutput) console.log(`Error: ${errorOutput.trim()}`);
        setTimeout(() => resolve(), 1000);
      }
    });

    killPort.on("error", (error) => {
      console.log(`âš ï¸  Error checking port: ${error.message}`);
      setTimeout(() => resolve(), 1000);
    });
  });
}

// Utility to find the first available port starting from basePort
async function findAvailablePort(
  basePort: number,
  maxAttempts = 10
): Promise<number> {
  for (let i = 0; i < maxAttempts; i++) {
    const port = basePort + i;
    const isFree = await new Promise<boolean>((resolve) => {
      const tester = net
        .createServer()
        .once("error", (err: any) => {
          if (err.code === "EADDRINUSE") resolve(false);
          else resolve(false);
        })
        .once("listening", () => {
          tester.close();
          resolve(true);
        })
        .listen(port, HOST);
    });
    if (isFree) return port;
  }
  throw new Error(
    `No available port found from ${basePort} to ${basePort + maxAttempts - 1}`
  );
}

// Refactored handleAIDiagnosis for 3-step flow
async function handleAIDiagnosis(req: express.Request, res: express.Response) {
  if (!process.env.OPENAI_API_KEY) {
    return res.status(500).json({
      success: false,
      message: "OpenAI API key not configured",
      error:
        "Please set OPENAI_API_KEY in your environment to enable AI analysis.",
    });
  }

  const {
    carType,
    carModel,
    mileage,
    lastServiceType,
    problemDescription,
    previousQuestions = [],
    previousAnswers = [],
    chatHistory = [],
    step,
    year,
  } = req.body;

  const carYear = year;

  // Step auto-detection if not provided
  let currentStep = step;
  if (!currentStep) {
    if (previousQuestions.length === 0 && previousAnswers.length === 0) {
      currentStep = "initial";
    } else if (previousQuestions.length < 3) {
      currentStep = "questions";
    } else {
      currentStep = "final";
    }
  }

  // Helper: Build chat history section
  function buildHistorySection() {
    let historySection = "";
    if (chatHistory.length > 0) {
      historySection = "\n\nØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:";
      chatHistory.forEach((msg: string, i: number) => {
        historySection += `\n${msg}`;
      });
    } else if (previousQuestions.length > 0) {
      historySection = "\n\nØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:";
      previousQuestions.forEach((q: string, i: number) => {
        const answer = previousAnswers[i] || "(Ø¨Ø¯ÙˆÙ† Ø¥Ø¬Ø§Ø¨Ø©)";
        historySection += `\nØ³Ø¤Ø§Ù„: ${q}\nØ¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${answer}`;
      });
    }
    return historySection;
  }

  // Step 1: Initial analysis + Generate initial smart questions
  if (currentStep === "initial") {
    const historySection = buildHistorySection();
    const preliminaryPrompt = `\nØ£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚ØŒ Ù‚Ø¯Ù… Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø£ÙˆÙ„ÙŠØ© ÙÙ‚Ø· Ø¹Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø©.\n\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø©:\n- Ø§Ù„Ù†ÙˆØ¹: ${carType}\n- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carModel}\n- Ø§Ù„Ù…Ù…Ø´Ù‰: ${mileage} ÙƒÙ…\n${
      lastServiceType ? `- Ø¢Ø®Ø± ØµÙŠØ§Ù†Ø©: ${lastServiceType}` : ""
    }\n${historySection}\n\nÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:\n${problemDescription}\n\nÙ…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:\n1. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ø¹Ø§Ù… Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©\n2. Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø³Ù…Ø§Ø¡ Ù‚Ø·Ø¹ ØºÙŠØ§Ø± Ù…Ø­Ø¯Ø¯Ø©\n3. Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø³Ø¹Ø§Ø± Ø£Ùˆ ØªÙƒØ§Ù„ÙŠÙ\n4. Ù„Ø§ ØªÙ‚Ø¯Ù… ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥ØµÙ„Ø§Ø­ Ù…ÙØµÙ„Ø©\n5. Ù„Ø§ ØªÙ‚ØªØ±Ø­ Ù…Ø±Ø§ÙƒØ² ØµÙŠØ§Ù†Ø© Ù…Ø­Ø¯Ø¯Ø©\n6. Ø±ÙƒØ² ÙÙ‚Ø· Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…\n7. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n\nÙ‡Ø°Ø§ ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ ÙÙ‚Ø·. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù…Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚Ø·Ø¹ ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø³ÙŠØªÙ… Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ©.`;

    try {
      // Get preliminary analysis
      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Øª Ø£ÙˆÙ„ÙŠØ© Ø¹Ø§Ù…Ø©.",
          },
          { role: "user", content: preliminaryPrompt },
        ],
        max_tokens: 600,
        temperature: 0.7,
      });
      const result = completion.choices[0]?.message?.content || "";

      // Generate initial smart questions, using the initial analysis as context
      const smartQuestionsPrompt = `Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n- Ù†ÙˆØ¹ Ø§Ù„Ø³ÙŠØ§Ø±Ø©: ${carType}\n- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carModel}\n- Ø³Ù†Ø© Ø§Ù„ØµÙ†Ø¹: ${
        carYear || "ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©"
      }\n- Ø§Ù„Ù…Ù…Ø´Ù‰: ${mileage}\n- ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: ${problemDescription}\n- Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ: ${result}\n\nØ§ÙƒØªØ¨ 3 Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© ØªØ³Ø§Ø¹Ø¯Ùƒ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚ Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø£ÙØ¶Ù„ Ø­Ù„.\n- Ø§Ø¨Ø¯Ø£ ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø¨Ø±Ù‚Ù… (1. 2. 3.)\n- Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\n- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚`;

      const questionsCompletion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© ÙˆÙ…Ø®ØµØµØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.",
          },
          { role: "user", content: smartQuestionsPrompt },
        ],
        max_tokens: 400,
        temperature: 0.8,
      });
      const questionsText =
        questionsCompletion.choices[0]?.message?.content?.trim() || "";

      // Parse up to 3 questions from the response
      let questionLines = questionsText
        .split("\n")
        .filter((line) => line.trim().match(/^\d+\./));

      // If no numbered questions, fallback to any non-empty lines
      if (questionLines.length === 0) {
        questionLines = questionsText
          .split("\n")
          .filter((line) => line.trim().length > 0);
      }

      // If still empty, force generic questions
      const genericQuestions = [
        "Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ù…Ø¤Ø®Ø±Ù‹Ø§ØŸ",
        "Ù‡Ù„ ØªØ¸Ù‡Ø± Ø£ÙŠ Ø£Ø¶ÙˆØ§Ø¡ ØªØ­Ø°ÙŠØ±ÙŠØ© Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŸ",
        "Ù‡Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ­Ø¯Ø« ÙÙŠ Ø¸Ø±ÙˆÙ Ù…Ø¹ÙŠÙ†Ø© ÙÙ‚Ø·ØŸ",
      ];
      if (questionLines.length === 0) {
        questionLines = genericQuestions.slice();
      }
      while (questionLines.length < 3) {
        questionLines.push(genericQuestions[questionLines.length]);
      }

      const followUpQuestions = questionLines
        .slice(0, 3)
        .map((line, index) => ({
          id: (index + 1).toString(),
          question: line.replace(/^\d+\.\s*/, "").trim(),
          type: "text",
          timestamp: new Date().toISOString(),
        }));

      console.log("Returning followUpQuestions:", followUpQuestions);

      return res.json({
        success: true,
        result: result.trim(),
        followUpQuestions,
        timestamp: new Date().toISOString(),
        note: "Preliminary analysis completed with smart questions",
      });
    } catch (err) {
      return res.status(500).json({
        success: false,
        message: "AI analysis failed",
        error: err instanceof Error ? err.message : String(err),
      });
    }
  }

  // Step 2: Generate 3 dynamic smart questions
  if (currentStep === "questions") {
    const historySection = buildHistorySection();
    const smartQuestionsPrompt = `Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚ØŒ Ø§Ø·Ø±Ø­ 3 Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø®ÙŠØµ.\n\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø©:\n- Ø§Ù„Ù†ÙˆØ¹: ${carType}\n- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carModel}\n- Ø§Ù„Ù…Ù…Ø´Ù‰: ${mileage} ÙƒÙ…\n${
      lastServiceType ? `- Ø¢Ø®Ø± ØµÙŠØ§Ù†Ø©: ${lastServiceType}` : ""
    }\n\nÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n${problemDescription}\n\nØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚:\n${historySection}\n\nÙ…Ø¹Ø±Ù Ø§Ù„Ø¬Ù„Ø³Ø©: ${
      Date.now().toString() + Math.random().toString(36).substr(2, 9)
    }\nØ§Ù„ÙˆÙ‚Øª: ${new Date().toISOString()}\n\nÙ…Ø·Ù„ÙˆØ¨:\n1. Ø§Ø·Ø±Ø­ 3 Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\n2. Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø®ØªÙ„ÙØ© ØªÙ…Ø§Ù…Ø§Ù‹\n3. Ø§Ø³ØªØ®Ø¯Ù… Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ø§Ù‹ ÙˆØªÙØµÙŠÙ„Ø§Ù‹\n4. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n5. Ø§ÙƒØªØ¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠ\n6. Ø§Ø¨Ø¯Ø£ ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø¨Ø±Ù‚Ù… (1. 2. 3.)\n7. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚\n8. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø¬ÙˆØ§Ù†Ø¨ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Ø£Ø¹Ø±Ø§Ø¶ØŒ ØªÙˆÙ‚ÙŠØªØŒ Ø¸Ø±ÙˆÙØŒ Ø¥Ù„Ø®)\n9. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹\n10. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„\n\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n1. Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„ÙˆÙ‚ÙˆØ¯ØŸ\n2. Ù‡Ù„ ØªØ¸Ù‡Ø± Ø£ÙŠ Ø£Ø¶ÙˆØ§Ø¡ ØªØ­Ø°ÙŠØ±ÙŠØ© Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŸ\n3. Ù‡Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ¸Ù‡Ø± ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ©ØŸ`;
    try {
      const questionsCompletion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© ÙˆÙ…Ø®ØµØµØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.",
          },
          { role: "user", content: smartQuestionsPrompt },
        ],
        max_tokens: 400,
        temperature: 0.8,
      });
      const questionsText =
        questionsCompletion.choices[0]?.message?.content?.trim() || "";
      // Parse up to 3 unique, new questions from the response
      const questionLines = questionsText
        .split("\n")
        .filter((line) => line.trim().match(/^\d+\./));
      const followUpQuestions = questionLines
        .map((line: string, index: number) => {
          const question = line.replace(/^\d+\.\s*/, "").trim();
          return {
            id: (previousQuestions.length + index + 1).toString(),
            question: question,
            type: "text",
            timestamp: new Date().toISOString(),
          };
        })
        .filter(
          (q: { question: string }) => !previousQuestions.includes(q.question)
        )
        .slice(0, 3 - previousQuestions.length);
      if (followUpQuestions.length > 0) {
        return res.json({
          success: true,
          result: "",
          followUpQuestions,
          timestamp: new Date().toISOString(),
          note: "Smart follow-up questions generated",
        });
      } else {
        return res.json({
          success: true,
          result: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ©.",
          followUpQuestions: [],
          timestamp: new Date().toISOString(),
          note: "No additional questions needed",
        });
      }
    } catch (error) {
      return res.status(500).json({
        success: false,
        message: "Failed to generate smart questions",
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  // Step 3: Final detailed analysis
  if (currentStep === "final") {
    const historySection = buildHistorySection();
    const detailedPrompt = `\nØ£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª Ù…Ø­ØªØ±Ù ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ù‹Ø§ ØªÙ‚Ù†ÙŠÙ‹Ø§ Ù…ÙØµÙ„Ù‹Ø§ ÙˆÙ…ØªÙ‚Ø¯Ù…Ù‹Ø§.\n\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø©:\n- Ø§Ù„Ù†ÙˆØ¹: ${carType}\n- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carModel}\n- Ø§Ù„Ù…Ù…Ø´Ù‰: ${mileage} ÙƒÙ…\n${
      lastServiceType ? `- Ø¢Ø®Ø± ØµÙŠØ§Ù†Ø©: ${lastServiceType}` : ""
    }\n${historySection}\n\nÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n${problemDescription}\n\nÙ…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:\n1. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ù†ÙŠ Ù…ÙØµÙ„ ÙˆÙ…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø´ÙƒÙ„Ø©\n2. Ø§Ø°ÙƒØ± Ø£Ø³Ù…Ø§Ø¡ Ù‚Ø·Ø¹ Ø§Ù„ØºÙŠØ§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n3. Ù‚Ø¯Ù… ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø£Ø³Ø¹Ø§Ø± Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„ÙƒÙˆÙŠØªÙŠ (Ø¯.Ùƒ)\n4. Ø§Ø°ÙƒØ± ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙˆØ§Ù„Ø®Ø·ÙˆØ§Øª\n5. Ø§Ù‚ØªØ±Ø­ Ù…Ø±Ø§ÙƒØ² ØµÙŠØ§Ù†Ø© Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª Ù…Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©\n6. Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ù†Ø§Ø®ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª\n7. Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø®ÙŠØµ\n8. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n\nÙ‡Ø°Ø§ ØªØ­Ù„ÙŠÙ„ Ù†Ù‡Ø§Ø¦ÙŠ Ù…ÙØµÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.`;
    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª Ù…Ø­ØªØ±Ù ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª Ù…Ø¹ Ø®Ø¨Ø±Ø© 20+ Ø³Ù†Ø©. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„ÙƒÙˆÙŠØªÙŠ (Ø¯.Ùƒ) Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±. Ø§Ø°ÙƒØ± Ù†ØµÙŠØ­ØªÙŠÙ† ÙÙ‚Ø· Ù„Ù„ÙˆÙ‚Ø§ÙŠØ©.",
          },
          { role: "user", content: detailedPrompt },
        ],
        max_tokens: 1000,
        temperature: 0.7,
      });
      const result = completion.choices[0]?.message?.content || "";
      return res.json({
        success: true,
        result: result.trim(),
        followUpQuestions: [],
        timestamp: new Date().toISOString(),
        note: "AI-generated enhanced analysis",
      });
    } catch (err) {
      return res.status(500).json({
        success: false,
        message: "AI analysis failed",
        error: err instanceof Error ? err.message : String(err),
      });
    }
  }

  // Fallback: unknown step
  return res.status(400).json({
    success: false,
    message: "Invalid step or insufficient data for analysis.",
  });
}

// Proxy /api/analyze to /api/analyze-guided for compatibility
app.post(
  "/api/analyze",
  [
    body("carType").notEmpty().withMessage("Car type is required"),
    body("carModel").notEmpty().withMessage("Car model is required"),
    body("mileage").notEmpty().withMessage("Mileage is required"),
    body("problemDescription")
      .notEmpty()
      .withMessage("Problem description is required"),
  ],
  async (req: express.Request, res: express.Response) => {
    // Validation
    const errors = validationResult(req);
    if (!errors.isEmpty()) {
      return res.status(400).json({
        success: false,
        message: "Validation failed",
        errors: errors.array(),
      });
    }
    return handleAIDiagnosis(req, res);
  }
);

// Direct route for /api/analyze-guided
app.post(
  "/api/analyze-guided",
  [
    body("carType").notEmpty().withMessage("Car type is required"),
    body("carModel").notEmpty().withMessage("Car model is required"),
    body("mileage").notEmpty().withMessage("Mileage is required"),
    body("problemDescription")
      .notEmpty()
      .withMessage("Problem description is required"),
  ],
  async (req: express.Request, res: express.Response) => {
    console.log("=== /api/analyze-guided endpoint hit ===");
    // Check for validation errors
    const errors = validationResult(req);
    if (!errors.isEmpty()) {
      return res.status(400).json({
        success: false,
        message: "Validation failed",
        errors: errors.array(),
      });
    }
    return handleAIDiagnosis(req, res);
  }
);

// Generate additional smart follow-up questions
app.post(
  "/api/generate-questions",
  [
    body("carDetails").notEmpty().withMessage("Car details are required"),
    body("problemDescription")
      .notEmpty()
      .withMessage("Problem description is required"),
    body("previousQuestions")
      .isArray()
      .withMessage("Previous questions must be an array"),
    body("previousAnswers")
      .isArray()
      .withMessage("Previous answers must be an array"),
    body("chatHistory")
      .optional()
      .isArray()
      .withMessage("Chat history must be an array"),
  ],
  async (req: express.Request, res: express.Response) => {
    console.log("=== /api/generate-questions endpoint hit ===");
    try {
      // Check for validation errors
      const errors = validationResult(req);
      if (!errors.isEmpty()) {
        return res.status(400).json({
          success: false,
          message: "Validation failed",
          errors: errors.array(),
        });
      }

      const {
        carDetails,
        problemDescription,
        previousQuestions,
        previousAnswers,
        chatHistory = [],
      } = req.body;

      console.log(
        "Generating additional smart questions based on previous answers"
      );

      // Check if OpenAI API key is configured
      if (!process.env.OPENAI_API_KEY) {
        return res.status(500).json({
          success: false,
          message: "OpenAI API key not configured",
          error:
            "Please configure OpenAI API key for smart questions generation",
        });
      }

      // Create dynamic prompt combining problemDescription and chatHistory
      const sessionId =
        Date.now().toString() + Math.random().toString(36).substr(2, 9);

      // Build comprehensive chat history
      let fullChatHistory = "";
      if (chatHistory.length > 0) {
        fullChatHistory = "\n\nØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙƒØ§Ù…Ù„:";
        chatHistory.forEach((entry: any, index: number) => {
          fullChatHistory += `\n${index + 1}. ${entry.message || entry}`;
        });
      }

      // Build previous Q&A section
      let previousQASection = "";
      if (previousQuestions.length > 0) {
        previousQASection = "\n\nØ§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:";
        previousQuestions.forEach((q: any, index: number) => {
          const answer = previousAnswers[index] || "(Ø¨Ø¯ÙˆÙ† Ø¥Ø¬Ø§Ø¨Ø©)";
          previousQASection += `\nØ³Ø¤Ø§Ù„: ${q.question || q}\nØ¥Ø¬Ø§Ø¨Ø©: ${answer}`;
        });
      }

      const additionalQuestionsPrompt = `
Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ø§Ø·Ø±Ø­ 3 Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø®ÙŠØµ.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø©:
- Ø§Ù„Ù†ÙˆØ¹: ${carDetails.carType}
- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carDetails.carModel}
- Ø§Ù„Ù…Ù…Ø´Ù‰: ${carDetails.mileage} ÙƒÙ…
${
  carDetails.lastServiceType ? `- Ø¢Ø®Ø± ØµÙŠØ§Ù†Ø©: ${carDetails.lastServiceType}` : ""
}

Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©:
${problemDescription}${fullChatHistory}${previousQASection}

Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ù„Ø³Ø©: ${sessionId}
Ø§Ù„ÙˆÙ‚Øª: ${new Date().toISOString()}

Ù…Ø·Ù„ÙˆØ¨:
1. Ø§Ø·Ø±Ø­ 3 Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
2. Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø®ØªÙ„ÙØ© ØªÙ…Ø§Ù…Ø§Ù‹
3. Ø§Ø³ØªØ®Ø¯Ù… Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ø§Ù‹ ÙˆØªÙØµÙŠÙ„Ø§Ù‹
4. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
5. Ø§ÙƒØªØ¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠ
6. Ø§Ø¨Ø¯Ø£ ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø¨Ø±Ù‚Ù… (1. 2. 3.)
7. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚
8. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø¬ÙˆØ§Ù†Ø¨ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Ø£Ø¹Ø±Ø§Ø¶ØŒ ØªÙˆÙ‚ÙŠØªØŒ Ø¸Ø±ÙˆÙØŒ Ø¥Ù„Ø®)
9. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
10. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„

Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„ÙˆÙ‚ÙˆØ¯ØŸ
2. Ù‡Ù„ ØªØ¸Ù‡Ø± Ø£ÙŠ Ø£Ø¶ÙˆØ§Ø¡ ØªØ­Ø°ÙŠØ±ÙŠØ© Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŸ
3. Ù‡Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ¸Ù‡Ø± ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ©ØŸ`;

      // Generate additional questions using OpenAI
      const questionsCompletion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© ÙˆÙ…Ø®ØµØµØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.",
          },
          { role: "user", content: additionalQuestionsPrompt },
        ],
        max_tokens: 300,
        temperature: 0.8,
      });

      const questionsText =
        questionsCompletion.choices[0]?.message?.content || "";

      // Parse the questions from the response
      let questionLines = questionsText
        .split("\n")
        .filter((line) => line.trim().match(/^\d+\./));

      // If no numbered questions, fallback to any non-empty lines
      if (questionLines.length === 0) {
        questionLines = questionsText
          .split("\n")
          .filter((line) => line.trim().length > 0);
      }

      // If still empty, force generic questions
      const genericQuestions = [
        "Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ù…Ø¤Ø®Ø±Ù‹Ø§ØŸ",
        "Ù‡Ù„ ØªØ¸Ù‡Ø± Ø£ÙŠ Ø£Ø¶ÙˆØ§Ø¡ ØªØ­Ø°ÙŠØ±ÙŠØ© Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŸ",
        "Ù‡Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ­Ø¯Ø« ÙÙŠ Ø¸Ø±ÙˆÙ Ù…Ø¹ÙŠÙ†Ø© ÙÙ‚Ø·ØŸ",
      ];
      if (questionLines.length === 0) {
        questionLines = genericQuestions.slice();
      }
      while (questionLines.length < 3) {
        questionLines.push(genericQuestions[questionLines.length]);
      }

      const additionalQuestions = questionLines
        .slice(0, 3)
        .map((line, index) => {
          const question = line.replace(/^\d+\.\s*/, "").trim();
          return {
            id: (previousQuestions.length + index + 1).toString(),
            question: question,
            type: "text",
            timestamp: new Date().toISOString(),
          };
        });

      console.log(
        "Generated additional smart questions:",
        additionalQuestions.map((q) => q.question)
      );

      return res.json({
        success: true,
        questions: additionalQuestions,
        timestamp: new Date().toISOString(),
        note: "AI-generated additional questions based on previous answers",
      });
    } catch (error: any) {
      console.error("Error generating additional questions:", error);
      return res.status(500).json({
        success: false,
        message: "Failed to generate additional questions",
        error: error?.message || "Unknown error",
      });
    }
  }
);

// Follow-up analysis endpoint
app.post(
  "/api/analyze-followup",
  [
    body("initialAnalysis")
      .notEmpty()
      .withMessage("Initial analysis is required"),
    body("followUpAnswers")
      .isArray()
      .withMessage("Follow-up answers must be an array"),
    body("followUpQuestions")
      .isArray()
      .withMessage("Follow-up questions must be an array"),
    body("carDetails").notEmpty().withMessage("Car details are required"),
    body("image").optional().isBoolean().withMessage("Image must be a boolean"),
  ],
  async (req: express.Request, res: express.Response) => {
    console.log("=== /api/analyze-followup endpoint hit ===");
    try {
      // Check for validation errors
      const errors = validationResult(req);
      if (!errors.isEmpty()) {
        return res.status(400).json({
          success: false,
          message: "Validation failed",
          errors: errors.array(),
        });
      }

      const {
        initialAnalysis,
        followUpAnswers,
        followUpQuestions,
        carDetails,
        image,
      } = req.body;

      // Log all received inputs for debugging
      console.log("Received carDetails:", carDetails);
      console.log("Received initialAnalysis:", initialAnalysis);
      console.log("Received followUpQuestions:", followUpQuestions);
      console.log("Received followUpAnswers:", followUpAnswers);

      // Compose the detailed analysis prompt in Arabic with Kuwait-specific pricing
      const detailedPrompt = `
Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª Ù…Ø­ØªØ±Ù ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ù‹Ø§ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§ Ù…ÙØµÙ„Ù‹Ø§ ÙˆÙ…ØªÙ‚Ø¯Ù…Ù‹Ø§:

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø©:
- Ø§Ù„Ù†ÙˆØ¹: ${carDetails.brand}
- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${carDetails.model}
- Ø§Ù„Ø³Ù†Ø©: ${carDetails.year}
- Ø§Ù„Ù…Ù…Ø´Ù‰: ${carDetails.mileage} ÙƒÙ…

ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:
${carDetails.problemDescription}

Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ:
${initialAnalysis}

Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
${
  Array.isArray(followUpQuestions) &&
  Array.isArray(followUpAnswers) &&
  followUpQuestions.length === followUpAnswers.length
    ? followUpQuestions
        .map(
          (q, i) => `Ø³${i + 1}: ${q.question}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: ${followUpAnswers[i]}`
        )
        .join("\n")
    : "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ§Ø­Ø©."
}

Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
1. Ø§Ø¨Ø¯Ø£ Ø¨Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„Ø³ÙŠØ§Ø±Ø© (Ø§Ø°ÙƒØ± Ø§Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ø³Ù†Ø©)
2. Ù‚Ø¯Ù… Ù…Ù„Ø®Øµ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø§ Ø³Ø¨Ù‚
3. Ø­Ø¯Ø¯ Ù‚Ø·Ø¹ Ø§Ù„ØºÙŠØ§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø¯Ù‚Ø©
4. Ù‚Ø¯Ù… ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø£Ø³Ø¹Ø§Ø± Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„ÙƒÙˆÙŠØªÙŠ (Ø¯.Ùƒ) Ø­Ø³Ø¨ Ù†ÙˆØ¹ ÙˆÙ…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ø±Ø©
5. Ø§Ø°ÙƒØ± ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ÙˆØ§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
6. Ø§Ù‚ØªØ±Ø­ Ù…Ø±Ø§ÙƒØ² ØµÙŠØ§Ù†Ø© Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª
7. Ù‚Ø¯Ù… Ù†ØµÙŠØ­ØªÙŠÙ† ÙÙ‚Ø· Ù„Ù„ÙˆÙ‚Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
8. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©

ØµÙŠØºØ© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:
---
ğŸ“ Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø³ÙŠØ§Ø±Ø©
ğŸ” Ù…Ù„Ø®Øµ Ø§Ù„ØªØ´Ø®ÙŠØµ
ğŸ§© Ù‚Ø·Ø¹ Ø§Ù„ØºÙŠØ§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
ğŸ’µ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± (Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„)
ğŸ”§ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥ØµÙ„Ø§Ø­
ğŸ§° Ù…Ø±Ø§ÙƒØ² Ø§Ù„ØµÙŠØ§Ù†Ø©
âœ… Ù†ØµØ§Ø¦Ø­ ÙˆÙ‚Ø§Ø¦ÙŠØ©
---

Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ¬Ø¹Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠÙ‹Ø§ ÙˆØ´Ø®ØµÙŠÙ‹Ø§.
      `;

      console.log("Calling OpenAI API for follow-up analysis...");

      // Call OpenAI GPT
      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ Ø³ÙŠØ§Ø±Ø§Øª Ù…Ø­ØªØ±Ù ÙÙŠ Ø§Ù„ÙƒÙˆÙŠØª Ù…Ø¹ Ø®Ø¨Ø±Ø© 20+ Ø³Ù†Ø©. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„ÙƒÙˆÙŠØªÙŠ (Ø¯.Ùƒ) Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±. Ø§Ø°ÙƒØ± Ù†ØµÙŠØ­ØªÙŠÙ† ÙÙ‚Ø· Ù„Ù„ÙˆÙ‚Ø§ÙŠØ©.",
          },
          { role: "user", content: detailedPrompt },
        ],
        max_tokens: 1000,
        temperature: 0.7,
      });

      const aiResponse =
        completion.choices[0]?.message?.content?.trim() ||
        "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.";

      console.log("OpenAI API call successful for follow-up analysis");

      return res.json({
        success: true,
        result: aiResponse,
        timestamp: new Date().toISOString(),
        note: "AI-generated enhanced analysis",
      });
    } catch (error: any) {
      console.error("Error in follow-up analysis:", error);

      // Check if it's an OpenAI API error
      if (error?.response?.status === 401) {
        return res.status(500).json({
          success: false,
          message: "OpenAI API key is invalid or expired",
          error: "Please check your OpenAI API key configuration",
        });
      } else if (error?.response?.status === 429) {
        return res.status(500).json({
          success: false,
          message: "OpenAI API rate limit exceeded",
          error: "Please try again later",
        });
      } else if (
        error?.code === "ENOTFOUND" ||
        error?.code === "ECONNREFUSED"
      ) {
        return res.status(500).json({
          success: false,
          message: "Cannot connect to OpenAI API",
          error: "Please check your internet connection",
        });
      }

      return res.status(500).json({
        success: false,
        message: "Internal server error",
        error: error?.response?.data || error.message || error,
      });
    }
  }
);

// 404 handler
app.use("*", (req, res) => {
  console.log(
    `[DEBUG] 404 - Route not found: ${req.method} ${req.originalUrl}`
  );
  res.status(404).json({
    success: false,
    message: "Route not found",
  });
});

// Global error handler
app.use(
  (
    error: any,
    req: express.Request,
    res: express.Response,
    next: express.NextFunction
  ) => {
    console.error("Global error:", error);
    res.status(500).json({
      success: false,
      message: "Internal server error",
    });
  }
);

// Robust server start: try ports until one is free, never crash on EADDRINUSE
async function startServer() {
  const basePort = Number(process.env.PORT) || 8001;
  const maxAttempts = 10;
  let port = basePort;
  let attempts = 0;
  let serverStarted = false;

  while (!serverStarted && attempts < maxAttempts) {
    try {
      const server = app.listen(port, HOST, () => {
        if (port !== basePort) {
          console.log(
            `âš ï¸  Port ${basePort} was in use. Started server on next available port: ${port}`
          );
        } else {
          console.log(`ğŸš€ Server running on ${HOST}:${port}`);
        }
        console.log(`ğŸ“Š Health check: http://localhost:${port}/health`);
        console.log(`ğŸ”— API Base URL: http://localhost:${port}/api`);
        console.log(`ğŸŒ LAN Access: http://${HOST}:${port}`);
        console.log(
          `ğŸ¯ Target endpoint: http://192.168.8.149:${port}/api/analyze-guided`
        );
      });
      // If we get here, server started successfully
      serverStarted = true;
      process.env.PORT = port.toString();
    } catch (err: any) {
      if (err.code === "EADDRINUSE") {
        console.log(`âš ï¸  Port ${port} in use, trying next port...`);
        port++;
        attempts++;
      } else {
        console.error("âŒ Failed to start server:", err);
        break;
      }
    }
  }
  if (!serverStarted) {
    console.error(
      `âŒ Could not start server on any port from ${basePort} to ${
        basePort + maxAttempts - 1
      }`
    );
  }
}

startServer();

export default app;
